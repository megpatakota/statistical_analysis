{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hotels.com Customer Churn Analysis\n",
        "## Main Analysis Notebook\n",
        "\n",
        "This notebook orchestrates the complete churn analysis pipeline using modular Python code from the `src/` folder.\n",
        "\n",
        "**Features:**\n",
        "- ✅ Loads saved models if they exist (no retraining needed)\n",
        "- ✅ Generates all visualizations\n",
        "- ✅ Complete statistical analysis\n",
        "- ✅ Customer risk scoring\n",
        "\n",
        "**Run all cells sequentially to execute the full analysis.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import custom modules from src folder\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from src import config\n",
        "from src import data_loader\n",
        "from src import visualizations\n",
        "from src import statistical_tests\n",
        "from src import models\n",
        "\n",
        "# Setup plot style\n",
        "config.setup_plot_style()\n",
        "\n",
        "print(\"✓ All modules imported successfully\")\n",
        "print(\"✓ Plot style configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = data_loader.load_data()\n",
        "data_loader.get_data_summary(df)\n",
        "\n",
        "df_processed = data_loader.preprocess_data(df)\n",
        "customer_df = data_loader.aggregate_to_customer_level(df_processed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overall churn distribution\n",
        "visualizations.plot_churn_distribution(df_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Churn by categorical variables\n",
        "print(\"Customer Type Analysis:\")\n",
        "customer_type_analysis = visualizations.plot_churn_by_category(df_processed, 'customer_type', 'Customer Type')\n",
        "\n",
        "print(\"\\nLoyalty Tier Analysis:\")\n",
        "print(\"  0 = Not a member, 1 = Base member, 2 = Silver/Gold member\")\n",
        "loyalty_analysis = visualizations.plot_churn_by_category(df_processed, 'loyalty_tier', 'Loyalty Tier')\n",
        "\n",
        "print(\"\\nPlatform Analysis:\")\n",
        "platform_analysis = visualizations.plot_churn_by_category(df_processed, 'platform', 'Platform')\n",
        "\n",
        "print(\"\\nMarketing Channel Analysis:\")\n",
        "marketing_analysis = visualizations.plot_churn_by_category(df_processed, 'marketing_channel', 'Marketing Channel', figsize=(12, 6))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Binary flags analysis\n",
        "visualizations.plot_binary_flags(df_processed, config.BINARY_FLAGS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerical feature analysis\n",
        "statistical_tests.calculate_mean_comparison(df_processed, config.NUMERICAL_COLS)\n",
        "visualizations.plot_numerical_distributions(df_processed, config.NUMERICAL_COLS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "visualizations.plot_correlation_heatmap(df_processed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Statistical Significance Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical significance tests\n",
        "ttest_results = statistical_tests.perform_ttest(df_processed, config.NUMERICAL_COLS)\n",
        "\n",
        "chi_square_cols = ['customer_type', 'loyalty_tier', 'platform', 'marketing_channel',\n",
        "                   'coupon_flag', 'pay_now_flag', 'cancel_flag']\n",
        "chi_square_results = statistical_tests.perform_chi_square_tests(df_processed, chi_square_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Model Training and Evaluation\n",
        "\n",
        "**Note:** If models already exist in `results/` folder, they will be loaded instead of retraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features\n",
        "X, y, feature_cols, model_df_dummies = models.prepare_features(customer_df)\n",
        "\n",
        "# Check if models exist\n",
        "models_exist_flag = models.models_exist()\n",
        "\n",
        "if models_exist_flag:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"LOADING EXISTING MODELS (Skipping training)\")\n",
        "    print(\"=\"*60)\n",
        "    saved_models = models.load_models()\n",
        "    \n",
        "    if saved_models:\n",
        "        lr_model = saved_models['logistic_regression']\n",
        "        rf_model = saved_models['random_forest']\n",
        "        gb_model = saved_models['gradient_boosting']\n",
        "        scaler = saved_models['scaler']\n",
        "        feature_cols = saved_models['feature_cols']\n",
        "        \n",
        "        # Split data for evaluation\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.25, random_state=42, stratify=y\n",
        "        )\n",
        "        X_train_scaled = scaler.transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Get predictions\n",
        "        y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "        y_prob_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "        y_pred_rf = rf_model.predict(X_test)\n",
        "        y_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "        y_pred_gb = gb_model.predict(X_test)\n",
        "        y_prob_gb = gb_model.predict_proba(X_test)[:, 1]\n",
        "        \n",
        "        print(\"\\n✓ Models loaded successfully!\")\n",
        "        training_needed = False\n",
        "    else:\n",
        "        training_needed = True\n",
        "else:\n",
        "    training_needed = True\n",
        "\n",
        "if training_needed:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING NEW MODELS\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if training_needed:\n",
        "    # Split and scale data\n",
        "    X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler = models.split_and_scale_data(X, y)\n",
        "    \n",
        "    # Train Logistic Regression\n",
        "    lr_model, y_pred_lr, y_prob_lr = models.train_logistic_regression(\n",
        "        X_train_scaled, y_train, X_test_scaled, y_test\n",
        "    )\n",
        "    \n",
        "    # Get odds ratios\n",
        "    odds_ratios = models.get_logistic_regression_odds_ratios(lr_model, feature_cols)\n",
        "    \n",
        "    # Visualize LR coefficients\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "    from src.config import COLORS\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    top_features = odds_ratios.head(15)\n",
        "    colors = [COLORS['positive'] if c > 0 else COLORS['negative'] for c in top_features['Coefficient']]\n",
        "    ax.barh(range(len(top_features)), top_features['Coefficient'], color=colors, \n",
        "            edgecolor='white', linewidth=1)\n",
        "    ax.set_yticks(range(len(top_features)))\n",
        "    ax.set_yticklabels(top_features['Feature'])\n",
        "    ax.set_xlabel('Coefficient (Log Odds)', fontsize=12)\n",
        "    ax.set_title('Logistic Regression: Top 15 Feature Coefficients', fontweight='bold')\n",
        "    ax.axvline(x=0, color=COLORS['text'], linestyle='-', linewidth=1.5)\n",
        "    plt.tight_layout()\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⏭ Skipping training - using loaded models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if training_needed:\n",
        "    # Train Random Forest\n",
        "    rf_model, y_pred_rf, y_prob_rf = models.train_random_forest(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "    \n",
        "    # Feature importance\n",
        "    print(\"\\n--- Random Forest Feature Importance ---\")\n",
        "    rf_importance = visualizations.plot_feature_importance(\n",
        "        feature_cols, rf_model.feature_importances_,\n",
        "        title='Random Forest: Top 15 Feature Importance'\n",
        "    )\n",
        "else:\n",
        "    print(\"⏭ Skipping Random Forest training - using loaded model\")\n",
        "    # Still show feature importance from loaded model\n",
        "    rf_importance = visualizations.plot_feature_importance(\n",
        "        feature_cols, rf_model.feature_importances_,\n",
        "        title='Random Forest: Top 15 Feature Importance'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if training_needed:\n",
        "    # Train Gradient Boosting\n",
        "    gb_model, y_pred_gb, y_prob_gb = models.train_gradient_boosting(\n",
        "        X_train, y_train, X_test, y_test\n",
        "    )\n",
        "else:\n",
        "    print(\"⏭ Skipping Gradient Boosting training - using loaded model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Model Comparison and Risk Scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison - ROC curves and metrics\n",
        "metrics_comparison = visualizations.plot_model_comparison(\n",
        "    y_test, y_prob_lr, y_prob_rf, y_prob_gb,\n",
        "    y_pred_lr, y_pred_rf, y_pred_gb\n",
        ")\n",
        "\n",
        "# Confusion matrices\n",
        "print(\"\\n--- Confusion Matrices ---\")\n",
        "visualizations.plot_confusion_matrices(y_test, y_pred_lr, y_pred_rf, y_pred_gb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer Risk Scoring (using Random Forest - best model)\n",
        "customer_scores = models.score_customers(rf_model, X, model_df_dummies)\n",
        "visualizations.plot_risk_segmentation(customer_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"                      ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "┌─────────────────────────────────────────────────────────────────────┐\n",
        "│                        DATA SUMMARY                                  │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│  Total Bookings Analysed:         {len(df):>10,}                          │\n",
        "│  Unique Customers:                {len(customer_df):>10,}                          │\n",
        "│  Overall Churn Rate:              {df['churn_flag'].mean()*100:>10.2f}%                        │\n",
        "│  Date Range:                      {df['bk_date'].min()} to {df['bk_date'].max()}       │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│                        MODEL PERFORMANCE                             │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│  Best Model:                      Random Forest                      │\n",
        "│  ROC-AUC Score:                   {metrics_comparison.loc[metrics_comparison['Model'] == 'Random Forest', 'ROC-AUC'].values[0]:>10.4f}                        │\n",
        "│  Accuracy:                        {metrics_comparison.loc[metrics_comparison['Model'] == 'Random Forest', 'Accuracy'].values[0]:>10.4f}                        │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│                        KEY DRIVERS OF CHURN                          │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│  1. Customer Type (New vs Existing)                                  │\n",
        "│  2. Loyalty Tier (Non-member vs Member)                              │\n",
        "│  3. Cancellation Behaviour                                           │\n",
        "│  4. Website Engagement (Visit duration, pages viewed)                │\n",
        "│  5. Number of Previous Bookings                                      │\n",
        "└─────────────────────────────────────────────────────────────────────┘\n",
        "\"\"\")\n",
        "\n",
        "print(\"✅ Analysis Complete - Ready for presentation to leadership team\")\n",
        "print(f\"✅ Models saved to: results/\")\n",
        "print(f\"   • logistic_regression.pkl\")\n",
        "print(f\"   • random_forest.pkl\")\n",
        "print(f\"   • gradient_boosting.pkl\")\n",
        "print(f\"   • scaler.pkl\")\n",
        "print(f\"   • feature_cols.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
